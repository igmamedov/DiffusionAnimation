base_model_path: ./pretrained_weights/sd-image-variations-diffusers
checkpointing_steps: 2000
controlnet_openpose_path: ./pretrained_weights/control_v11p_sd15_openpose/diffusion_pytorch_model.bin
data:
  meta_paths:
  - ./data/fashion_meta.json
  sample_margin: 30
  train_bs: 2
  train_height: 768
  train_width: 768
enable_zero_snr: true
exp_name: stage1
image_encoder_path: ./pretrained_weights/sd-image-variations-diffusers/image_encoder
noise_offset: 0.05
noise_scheduler_kwargs:
  beta_end: 0.012
  beta_schedule: scaled_linear
  beta_start: 0.00085
  clip_sample: false
  num_train_timesteps: 1000
  steps_offset: 1
output_dir: ./exp_output
pose_guider_pretrain: true
resume_from_checkpoint: ''
save_model_epoch_interval: 5
seed: 12580
snr_gamma: 5.0
solver:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  adam_weight_decay: 0.01
  enable_xformers_memory_efficient_attention: true
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  learning_rate: 1.0e-05
  lr_scheduler: constant
  lr_warmup_steps: 1
  max_grad_norm: 1.0
  max_train_steps: 30000
  mixed_precision: fp16
  scale_lr: false
  use_8bit_adam: true
uncond_ratio: 0.1
vae_model_path: ./pretrained_weights/sd-vae-ft-mse
val:
  validation_steps: 200
weight_dtype: fp16
